# OttoDev

A lightweight local development assistant with AI chat and code generation capabilities. Think of it as a locally runnable version of bolt.diy with essential features for developers.

## ğŸš€ Features

- **AI Chat Interface**: Clean React-based chat UI with streaming responses
- **Code Generation**: Generate and preview code snippets with syntax highlighting
- **File Management**: Upload, view, and manage project files
- **Live Preview**: Real-time preview of generated HTML/CSS/JS
- **Local AI Integration**: Works with local Ollama models
- **Modern Stack**: React + TypeScript frontend, Express + TypeScript backend

## ğŸ—ï¸ Project Structure

```
ottodev/
â”œâ”€â”€ server.ts              # Main Express server
â”œâ”€â”€ routes/                # API routes
â”‚   â”œâ”€â”€ chat.ts           # Chat endpoints
â”‚   â”œâ”€â”€ upload.ts         # File upload
â”‚   â”œâ”€â”€ health.ts         # Health check
â”‚   â””â”€â”€ code.ts           # Code generation
â”œâ”€â”€ controllers/           # Business logic
â”œâ”€â”€ services/             # External integrations
â”œâ”€â”€ models/               # TypeScript interfaces
â”œâ”€â”€ utils/                # Utilities
â”œâ”€â”€ src/                  # React frontend
â”‚   â”œâ”€â”€ components/       # UI components
â”‚   â”œâ”€â”€ pages/           # Page components
â”‚   â”œâ”€â”€ hooks/           # Custom hooks
â”‚   â”œâ”€â”€ services/        # API services
â”‚   â””â”€â”€ utils/           # Frontend utilities
â”œâ”€â”€ uploads/             # File uploads
â””â”€â”€ generated/           # Generated code files
```

## ğŸ› ï¸ Prerequisites

1. **Node.js 20+** installed
2. **Ollama** running locally:
   ```bash
   ollama serve
   ```
3. **DeepSeek Coder model** installed:
   ```bash
   ollama pull deepseek-coder
   ```

## ğŸš€ Quick Start

1. **Install dependencies**:
   ```bash
   npm install
   ```

2. **Start development**:
   ```bash
   npm run dev
   ```

3. **Open your browser** to `http://localhost:3000`

## ğŸ“ Usage

### Chat with AI
- Type messages to get AI assistance
- Ask for code generation, explanations, or debugging help
- Responses stream in real-time

### Generate Code
- Request HTML, CSS, or JavaScript code
- Preview generated code instantly
- Save generated files to your project

### File Management
- Upload files (.txt, .md, .pdf, .zip)
- View and manage uploaded files
- Reference files in your conversations

## ğŸ”§ Configuration

Environment variables:
- `OLLAMA_HOST` - Ollama server URL (default: `http://localhost:11434`)
- `MODEL` - Ollama model name (default: `deepseek-coder`)
- `PORT` - Backend port (default: `3001`)

## ğŸƒâ€â™‚ï¸ Development

- `npm run dev` - Start both frontend and backend
- `npm run dev:server` - Backend only (port 3001)
- `npm run dev:client` - Frontend only (port 3000)
- `npm run build` - Build for production
- `npm run clean` - Clean build artifacts

## ğŸ¯ Roadmap

- [ ] Project templates
- [ ] Git integration
- [ ] Database schema generation
- [ ] API endpoint generation
- [ ] Component library
- [ ] Deployment helpers

---

**OttoDev** - Your local development companion ğŸ¤–